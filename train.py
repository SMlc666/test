import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader, random_split
import numpy as np
from pathlib import Path
import logging
from model import GomokuModel

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration ---
# Path to the data generated by data/process_sgf.py
DATA_PATH = Path("data/gomoku_data.npz")
MODEL_OUTPUT_PATH = Path("gomoku_model.onnx")
BOARD_SIZE = 15
LEARNING_RATE = 0.001
BATCH_SIZE = 64
EPOCHS = 20
VALIDATION_SPLIT = 0.1

def load_data(data_path):
    """Loads the preprocessed data from the .npz file."""
    if not data_path.exists():
        logging.error(f"Data file not found at {data_path}.")
        logging.error("Please run `python data/process_sgf.py` first to generate the training data.")
        return None
    
    try:
        # Your script saves data with keys 'states', 'policies', 'values'
        with np.load(data_path) as data:
            return {
                "states": data["states"],
                "policies": data["policies"],
                "values": data["values"],
            }
    except Exception as e:
        logging.error(f"Failed to load data from {data_path}: {e}")
        return None

def main():
    """Main function to run the model training."""
    
    # --- Device Configuration ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    # --- Load Data ---
    processed_data = load_data(DATA_PATH)
    if processed_data is None:
        return

    # The states from your script are (N, H, W, C). PyTorch needs (N, C, H, W).
    # We permute the dimensions: (0, 3, 1, 2)
    board_states = torch.from_numpy(processed_data["states"]).float().permute(0, 3, 1, 2)
    policy_targets = torch.from_numpy(processed_data["policies"]).float()
    # Ensure values are in the shape (N, 1) for the loss function
    value_targets = torch.from_numpy(processed_data["values"]).float().view(-1, 1)

    dataset = TensorDataset(board_states, policy_targets, value_targets)

    # --- Split Data ---
    val_size = int(len(dataset) * VALIDATION_SPLIT)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    logging.info(f"Training data size: {len(train_dataset)}")
    logging.info(f"Validation data size: {len(val_dataset)}")

    # --- Initialize Model, Optimizer, and Loss Functions ---
    # The model is already updated to accept 2 input channels
    model = GomokuModel(board_size=BOARD_SIZE).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # Loss for policy head (cross-entropy for classification)
    policy_loss_fn = nn.CrossEntropyLoss()
    # Loss for value head (mean squared error for regression)
    value_loss_fn = nn.MSELoss()

    # --- Training Loop ---
    for epoch in range(EPOCHS):
        model.train()
        total_train_loss = 0.0
        for boards, policies, values in train_loader:
            boards, policies, values = boards.to(device), policies.to(device), values.to(device)

            optimizer.zero_grad()

            # Forward pass
            policy_logits, value_preds = model(boards)

            # Calculate losses
            policy_loss = policy_loss_fn(policy_logits, policies)
            value_loss = value_loss_fn(value_preds, values)
            loss = policy_loss + value_loss # Combine losses

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # --- Validation Loop ---
        model.eval()
        total_val_loss = 0.0
        with torch.no_grad():
            for boards, policies, values in val_loader:
                boards, policies, values = boards.to(device), policies.to(device), values.to(device)
                policy_logits, value_preds = model(boards)
                policy_loss = policy_loss_fn(policy_logits, policies)
                value_loss = value_loss_fn(value_preds, values)
                loss = policy_loss + value_loss
                total_val_loss += loss.item()
        
        avg_val_loss = total_val_loss / len(val_loader)

        logging.info(f"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

    # --- Export to ONNX ---
    logging.info("Training finished. Exporting model to ONNX...")
    model.eval()
    # Create a dummy input with the correct shape (2 channels) for tracing
    dummy_input = torch.randn(1, 2, BOARD_SIZE, BOARD_SIZE, device=device)
    
    try:
        torch.onnx.export(
            model,
            dummy_input,
            MODEL_OUTPUT_PATH,
            input_names=['board_state'],
            output_names=['policy', 'value'],
            dynamic_axes={
                'board_state': {0: 'batch_size'},
                'policy': {0: 'batch_size'},
                'value': {0: 'batch_size'}
            },
            opset_version=11 # A reasonably modern opset version
        )
        logging.info(f"Model successfully exported to {MODEL_OUTPUT_PATH}")
    except Exception as e:
        logging.error(f"Failed to export model to ONNX: {e}")


if __name__ == "__main__":
    main()
